<head>
        <link rel="stylesheet" href="assets/style.css">

</head>


<h2>The Medical Model of Vulnerability Assessment</h2>
<p>
<i>Started writing: 8/21/2025</i>
</p>
<br>
<hr>
<br>
<p style="color: #333333; font-size: 1.0rem;"><i>
Disclaimer: I have always found human medicine particularly interesting and strive to write about it in the most respectful way. 
The analogies crafted in this entry are intended as a tool for understanding and should not diminish the severity or complexity of medical practice. 
Of course, cybersecurity is not anywhere near comparable in the stakes of human life with diagnostic medicine.
</i>
</p>

<br>
<br>
<p>
In medicine, a screening test might reveal a preliminary indicator, an early sign that something could be wrong. 
A preliminary indicator is not the same as a diagnosis. It simply signals that further investigation is worth considering. 
Sometimes the concern proves harmless once additional testing is performed. Other times, these additional tests confirm a condition that requires treatment. 
</p>
<br>
<p>
Vulnerability management follows a similar pattern. Automated scans and alerting mechanisms act like screening tests, as they favor coverage and speed. 
The cost of this breadth, however, is the noise that is generated by the results. 
Many “hits” are benign in context or technically accurate but operationally irrelevant when business context is added. 
Treating them as diagnoses creates waste in an otherwise preferably lean environment. 
These preliminary indicators raise suspicion without establishing proof. 
Examples include version disclosures that appear out of date, suspicious keywords associated with known vulnerabilities, or configurations that go against policy. 
These findings should trigger a response that weighs additional context, the likelihood of exploitation, and the potential impact of compromise. 
</p>
<br>
<br>
<p>
<u>Pretest Probability and Context</u>
</p>
<p>
Security teams have to weigh each indicator against additional environmental context, which can be compared to the way clinicians interpret preliminary indicators along with a patient’s health and family history. 
The same software version disclosure on a development machine carries a different weight on a production server hosting customer data. 
This should go beyond the type of server into data classification—is the data public, internal, or secret? Is the server part of a critical business function that directly and immediately impacts customers, or is it a system used only for internal operations? 
Another critical contextual factor is whether the system is part of an outdated legacy asset that should have been decommissioned entirely. 
The answers to these questions profoundly influence the pretest probability of a vulnerability being a genuine threat, and further help in assigning it to a risk category. 
Context also includes a system’s compensating controls, from the network firewall to the user access management hierarchy. 
A preliminary indicator that might be a high-priority risk in one environment could be considered a low-priority issue in another simply because of these layered defenses. 
This context shapes pretest probability, which in turn guides how aggressively the team tests to confirm the vulnerability. 
</p>
<br>
<br>
<p>
<u>Escalation and Confirmation</u>
</p>
<p>
In medicine, the decision to move from non-invasive screening to expensive, invasive confirmatory procedures involves balancing risk, cost, and necessity. 
A provider must discuss these options thoroughly with a patient before proceeding. 
Not all tests are definitive; some simply help rule out a condition, while others are highly accurate but not 100% conclusive. 
Definitive, invasive tests are typically pursued when the pretest probability is high and the potential impact of a misdiagnosis is severe. 
</p>
<br>
<p>
In cybersecurity, not every preliminary indicator justifies extensive manual testing or live exploitation attempts. 
Escalation decisions should be proportionate to the criticality of the asset, the credibility of the threat, and the potential business impact. 
For example, a version disclosure on a non-critical internal tool may be low priority for implementing a fix, while the same exact finding on an external-facing web server could require a large effort from the red team and developers to test and deploy a patch within a week. 
Understanding the context of an attack surface is crucial for accurately assessing risk. 
Additionally, escalation should follow a tiered approach, moving from non-invasive validation to more involved procedures. 
What this means is that a security team would start with the least disruptive methods to confirm a preliminary indicator, such as checking an asset’s configuration files or reviewing logs. 
If those steps are inconclusive, they might move to a more active but still controlled test, such as running a proof-of-concept exploit in a non-production environment. 
Live testing should be reserved for situations where the asset is critical and the threat is credible, typically requiring a formal Rules of Engagement (ROE) document and approval from security leadership for a controlled execution. 
The goal is to confirm the “diagnosis” with the least amount of risk.
</p>
<br>
<br>
<p>
<u>Management of False Positives/Negatives</u>
</p>
<p>
Screening inevitably produces both false positives and false negatives. Rather than just filtering them out, these results should be used to improve the diagnostic model. 
Teams should track metrics on how many preliminary indicators are confirmed versus dismissed so that altering can be tuned for greater accuracy. 
This process of continuous improvement can be compared to how medical professionals use long-term patient data and outcomes to refine screening guidelines. 
New understandings of a disease’s markers often leads to new recommendations for who should be screened and how it should be done. 
</p>
<br>
<p>
Managing these outcomes also means establishing clear triage protocols. 
A false positive should be categorized and documented rather than thrown out and forgotten. 
This allows the security team to identify patterns in benign alerts and tune (adjust) their scanning tools to reduce noise. 
A common example of this is if an address is consistently flagged but is proven harmless in context (normally by using business logic as context), security teams add an exception rule to prevent future alerting. 
Conversely, a false negative is an even more serious issue that is often discovered through penetration testing or during an active security incident. 
It’s nearly impossible to eliminate all false negatives due to the inherent limitations of automated tools and the almost-infinite permutations that manual testers would have to explore. 
Teams, however, can and should perform reviews of the original screening tool’s configuration to understand why a specific vulnerability may have been missed.
</p>
<br>
<br>

<p><u>Ethics</u></p>
<p>
Many confirmatory procedures in medicine carry their own risks. Similarly, security assessments must be looked at through an ethical lens. 
Aggressive testing in a production environment can cause data corruption, so many teams utilize non-production environments to perform this kind of testing. 
Data corruption can happen when a test overwrites or alters database entries, which can still happen in non-prod environments. 
It’s nearly impossible to create a perfect, identical copy of a production environment. 
When teams try to replicate the data for testing, they often use a subset of the production data or use data masking techniques. 
Both of these methods introduce risks. A test that works on the simplified data set in a non-prod setting might fail in production because of the complex data relationships or dependencies that weren’t copied over. 
If the data masking process isn’t perfect, it can break the referential integrity between the data sets, causing a mismatch that can lead to corruption. 
The logical structure of the data including its complex relationships and interactions is often harder to duplicate than the physical housing of the data. 
As a result, a test that relies on a specific data state or sequence of events might corrupt the data in the non-prod environment simply because that environment wasn’t built to handle those conditions.
</p>
<br>
<p>
You must also consider the analysts themselves, who can face burnout when constantly dealing with a high volume of alerts. 
Alert fatigue is a very real problem in both cybersecurity and in medicine, leading to analysts missing or ignoring warnings when exposed to a high volume of irrelevant or non-critical information. 
It creates a “boy who cried wolf” scenario where the sheer volume of low-priority or false-positive alerts desensitizes the team to genuine threats. 
The least invasive method that can reliably confirm or dismiss a marker should always be selected first. 
And just as a physician documents a diagnostic procedure, security teams must document their process, including the rationale behind testing decisions, to maintain accountability and transparency. 
This documentation is an ethical imperative because it provides a clear record of the decisions made and the risks taken. 
It helps to ensure that teams are not taking unnecessary risks and that any unintended consequences of a test can be traced back to the source. 
It also serves as a knowledge transfer mechanism for future team members looking to understand the context and history of past assessments and decisions. 
</p>
<br>
<br>
<p><u>Feedback Loops</u>
</p>
<p>
A diagnostic model is only as strong as its feedback loop. 
Each confirmed finding should update detection signatures. 
And each dismissed finding should update and refine tool configurations. 
In this way, the organization continuously learns from its preliminary indicators. 
In medicine, this happens when screening guidelines evolve. 
For example, large-scale studies often result in the refining of a screening population, like in the case of breast cancer screening, where recommendations have been adjusted over time for different age groups based on new data. 
In a similar vein, the findings from dropped preliminary indicators in security can encourage a team to adjust their alerting thresholds. 
</p>
<br>
<p>
Beyond just updating tools, this feedback loop should influence the broader security program. 
Confirmed vulnerabilities should lead to the development of new training modules for better configuration and secure development. 
This training should focus on the root causes of issues, generalizing them to a level that allows for the recognition of broader vulnerability patterns. 
By teaching these concepts (much like the layered structure of the CWE), teams can learn to identify related weaknesses and apply similar remediation techniques to prevent them. 
This ensures that the training isn’t so specific that it’s only useful for one instance, nor so broad that it’s not actionable or adaptable. 
Additionally, as a recurrent finding, a specific vulnerability might indicate a policy-level issue for the secure development lifecycle. 
However, policy alone will not remediate these issues, so follow-up and enforcement should happen in response to governance document updates. 
Similarly, dismissed findings and false positives can encourage risk-based patching for teams who need to deprioritize vulnerabilities that have a low pretest probability in their specific environment. 
This moves the organization from a reactive state of simply treating symptoms to a proactive state of preventing the “disease” from taking hold in the first place.
</p>
<br>
<br>
<p>
<u>Assets as Patients</u>
</p>
<p>
Ultimately, adopting a diagnostic model of vulnerability assessment represents one framework for how organizations approach risk. 
It encourages analysis beyond automated scans to an entirely risk-informed process that leverages business context and ethical considerations to methodize confirmation and remediation. 
The analogies presented here should be treated as a framework for strategic resource prioritization. 
This approach is rooted in the tenet of personalized medicine, which is an attempt to tailor treatment to the individual patient. 
In the same way, security teams need to move beyond a one-size-fits-all approach to vulnerability management. 
They must treat each asset as a unique “patient,” assessing its specific context, business criticality, and compensating controls to determine the appropriate level of investigation and remediation. 
</p>

